{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means classification\n",
    "\n",
    "K-means clustering is one of the basic unsupervised classification algorithms. Its objective is to group similar data points together (clusters), by following patterns. The first step is to define a number k, which represents the number of clusters that will be formed. The center of each cluster is called “centroid”. When analyzing data, the algorithm identifies k centroids and allocates each data point to the nearest cluster.\n",
    "At the beginning, the algorithm starts with a first group of random centroids, then it performs repetitive calculations to optimize their positions. It stops whether the pre-set number of iterations has been reached or if there is no change in the positions of the centers.\n",
    "In order to determine the In case of natural language processing, word2vec embeddings are used in order to determine the position of each word in the multidimensional space. \n",
    "The algorithm consists of several steps. Firstly, the corpus is transformed into array of real numbers and data is positioned in a multi-dimensional space. Then, two phases are repeated for a fixed number of iterations: cluster assignment and centroid move. The algorithm goes through each of the data points and depending on which cluster is closer, it assigns the data point to it. It then calculates the average of all the points in a cluster and moves the centroid to that average location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"NLP_clustering.ipynb\"\n",
    "\n",
    "# the second method: starting from the preprocessed title + metadata abstract\n",
    "# you can use k-means with a variable number of clusters to get text classification\n",
    "# and check the optimal number of clusters using 2 methods\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    csw = CatalogueServiceWeb('http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw')\n",
    "    \n",
    "    set_title = fes.PropertyIsLike('any', '')\n",
    "    filter_list = [set_title]\n",
    "\n",
    "    csw.getrecords2(constraints=filter_list, maxrecords=2000)\n",
    "\n",
    "    fmt = '{:*^64}'.format\n",
    "    print(fmt(' Catalog information '))\n",
    "    print(\"CSW version: {}\".format(csw.version))\n",
    "    print(\"Number of datasets available: {}\".format(len(csw.records.keys())))\n",
    "    print('\\n')\n",
    "\n",
    "    original_list_of_titles = []\n",
    "    preprocessed_list_of_titles = []\n",
    "    word2vec_number_list = []\n",
    "    \n",
    "    model, index2word_set = init_language_model()\n",
    "    \n",
    "    for rec in csw.records:\n",
    "        original_list_of_titles.append(csw.records[rec].title)\n",
    "        title = prepareDescription(csw.records[rec].title, keepwords)\n",
    "        if csw.records[rec].abstract != '':\n",
    "            abstract = prepareDescription(csw.records[rec].abstract, keepwords)\n",
    "            title = title + \" \" + abstract\n",
    "        \n",
    "        preprocessed_list_of_titles.append(title.split())\n",
    "\n",
    "    model = gensim.models.Word2Vec(preprocessed_list_of_titles, min_count = 1, size=32)\n",
    "    \n",
    "    X=[]\n",
    "    for sentence in preprocessed_list_of_titles:\n",
    "        X.append(sent_vectorizer(sentence, model))\n",
    "\n",
    "    # Run the K-means algorithm for a variable nunbers of cluster\n",
    "    # in order to get the optimal value\n",
    "    K = range(2, 21)\n",
    "    distortions = []\n",
    "    silhs = []\n",
    "    \n",
    "    max_k_silh = 1\n",
    "    max_k_cal = 1\n",
    "    max_silh = -1\n",
    "    max_cal = -1\n",
    "    \n",
    "    for k in K:\n",
    "        kclusterer = KMeansClusterer(k, distance=nltk.cluster.util.euclidean_distance, repeats=25, avoid_empty_clusters=True)\n",
    "        assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    "        kmeans = cluster.KMeans(n_clusters=k)\n",
    "        kmeans.fit(X)\n",
    "\n",
    "        labels = kmeans.labels_\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        \n",
    "        print (\"Cluster id labels for inputted data\")\n",
    "        print (labels)\n",
    "        print (\"Centroids data\")\n",
    "        print (centroids)\n",
    "\n",
    "        print (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\n",
    "        print (kmeans.score(X))\n",
    "\n",
    "        silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    "        \n",
    "        print (\"Silhouette_score: \")\n",
    "        print (silhouette_score)\n",
    "        \n",
    "        if (silhouette_score > max_silh):\n",
    "            max_silh = silhouette_score\n",
    "            max_k_silh = k\n",
    "            \n",
    "        harabaz_score = metrics.calinski_harabaz_score(X, labels)\n",
    "        if harabaz_score > max_cal:\n",
    "            max_cal = harabaz_score\n",
    "            max_k_cal = k\n",
    "\n",
    "        distortions.append(kmeans.inertia_)\n",
    "        silhs.append(silhouette_score)\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        from sklearn.manifold import TSNE\n",
    "\n",
    "        model = TSNE(n_components=2, random_state=0)\n",
    "        np.set_printoptions(suppress=True)\n",
    "\n",
    "        Y=model.fit_transform(X)\n",
    "\n",
    "\n",
    "        plt.scatter(Y[:, 0], Y[:, 1], c=assigned_clusters, s=290,alpha=.5)\n",
    "\n",
    "\n",
    "        for j in range(len(preprocessed_list_of_titles)):    \n",
    "            plt.annotate(assigned_clusters[j],xy=(Y[j][0], Y[j][1]),xytext=(0,0),textcoords='offset points')\n",
    "           #print (\"%s %s\" % (assigned_clusters[j],  preprocessed_list_of_titles[j]))\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    \"\"\"\n",
    "    Plot the distortions for the elbow method\n",
    "    \n",
    "    This method looks at the percentage of variance explained as a function of the number of clusters:\n",
    "    One should choose a number of clusters so that adding another cluster doesn't give much better modeling\n",
    "    of the data. More precisely, if one plots the percentage of variance explained by the clusters against the\n",
    "    number of clusters, the first clusters will add much information (explain a lot of variance), but at some point\n",
    "    the marginal gain will drop, giving an angle in the graph.\n",
    "    This is a visual solution, but it is often ambiguous and not very reliable.\n",
    "    \"\"\"\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method showing the optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    \"\"\"\n",
    "    A better approach is the silhoutte, which provides a succinct graphical representation of how well each\n",
    "    object has been classified.\n",
    "    The silhouette value  is a measure of how similar an object is to its own cluster (cohesion) compared to other\n",
    "    clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well\n",
    "    matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then\n",
    "    the clustering configuration is appropriate.\n",
    "    \"\"\"\n",
    "    plt.plot(K, silhs, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('Silhouette score')\n",
    "    plt.show()\n",
    "    \n",
    "    print('============================================================')\n",
    "    print('k max for silhouette: %d' %max_k_silh)\n",
    "    print('k max for calinski_harabaz_score: %d' %max_k_cal)\n",
    "    print('============================================================')\n",
    "\n",
    " \n",
    "    \"\"\"\n",
    "    print (\"Cluster id labels for inputted data\")\n",
    "    print (labels)\n",
    "    print (\"Centroids data\")\n",
    "    print (centroids)\n",
    "\n",
    "    print (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\n",
    "    print (kmeans.score(X))\n",
    "\n",
    "    silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    "\n",
    "    print (\"Silhouette_score: \")\n",
    "    print (silhouette_score)\n",
    "\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    model = TSNE(n_components=NUM_CLUSTERS, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "\n",
    "    Y=model.fit_transform(X)\n",
    "\n",
    "\n",
    "    plt.scatter(Y[:, 0], Y[:, 1], c=assigned_clusters, s=290,alpha=.5)\n",
    "\n",
    "\n",
    "    for j in range(len(preprocessed_list_of_titles)):    \n",
    "       plt.annotate(assigned_clusters[j],xy=(Y[j][0], Y[j][1]),xytext=(0,0),textcoords='offset points')\n",
    "       print (\"%s %s\" % (assigned_clusters[j],  preprocessed_list_of_titles[j]))\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    word_vectors = model.wv.syn0\n",
    "    num_clusters = 4\n",
    "    kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "    idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "    word_centroid_map = dict(zip( model.wv.index2word, idx ))\n",
    "    \n",
    "    for cluster in range(0,num_clusters):\n",
    "        print(\"\\nCluster %d\" % cluster)\n",
    "        # Find all of the words for that cluster number, and print them out\n",
    "        words = []\n",
    "        v = list(word_centroid_map.values())\n",
    "        for i in range(len(v)):\n",
    "            if(v[i] == cluster ):mes\n",
    "                k = list(word_centroid_map.keys()) \n",
    "                words.append(k[i])\n",
    "        print(words)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"NLP_clustering.ipynb\"\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Phrases\n",
    "\n",
    "#based on k-means, you can get the optimal number of clusters based on the previous information\n",
    "#starting from it, using the metadata records already classified, you can determine the most important\n",
    "#words in each topic, using the bag of words approach\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    csw = CatalogueServiceWeb('http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw')\n",
    "    \n",
    "    set_title = fes.PropertyIsLike('any', '')#SEARCH_QUERY)\n",
    "    filter_list = [set_title]\n",
    "\n",
    "    csw.getrecords2(constraints=filter_list, maxrecords=2000)\n",
    "\n",
    "    fmt = '{:*^64}'.format\n",
    "    print(fmt(' Catalog information '))\n",
    "    print(\"CSW version: {}\".format(csw.version))\n",
    "    print(\"Number of datasets available: {}\".format(len(csw.records.keys())))\n",
    "    print('\\n')\n",
    "\n",
    "    original_list_of_titles = []\n",
    "    preprocessed_list_of_titles = []\n",
    "    identifiers = []\n",
    "    word2vec_number_list = []\n",
    "    \n",
    "    for rec in csw.records:\n",
    "        title = prepareDescription(csw.records[rec].title, keepwords)\n",
    "        identifiers.append(csw.records[rec].identifier)\n",
    "        if csw.records[rec].abstract != '':\n",
    "            abstract = prepareDescription(csw.records[rec].abstract, keepwords)\n",
    "            title = title + \" \" + abstract\n",
    "        \n",
    "        preprocessed_list_of_titles.append(word_tokenize(title))\n",
    "\n",
    "\n",
    "    assigned_clusters = [-1] * len(preprocessed_list_of_titles)\n",
    "    \n",
    "    while 1:\n",
    "        model = gensim.models.Word2Vec(preprocessed_list_of_titles, min_count = 1, size=32)\n",
    "\n",
    "        X=[]\n",
    "        for sentence in preprocessed_list_of_titles:\n",
    "            X.append(sent_vectorizer(sentence, model))\n",
    "\n",
    "        k = 2\n",
    "        kclusterer = KMeansClusterer(k, distance=nltk.cluster.util.cosine_distance, repeats=25, avoid_empty_clusters=True)\n",
    "        assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    "        kmeans = cluster.KMeans(n_clusters=k)\n",
    "        kmeans.fit(X)\n",
    "\n",
    "        labels = kmeans.labels_\n",
    "        centroids = kmeans.cluster_centers_\n",
    "\n",
    "        model = TSNE(n_components=2, random_state=0)\n",
    "        np.set_printoptions(suppress=True)\n",
    "\n",
    "        Y=model.fit_transform(X)\n",
    "\n",
    "        plt.scatter(Y[:, 0], Y[:, 1], c=assigned_clusters, s=290,alpha=.5)\n",
    "\n",
    "\n",
    "        for j in range(len(preprocessed_list_of_titles)):\n",
    "            plt.annotate(assigned_clusters[j],xy=(Y[j][0], Y[j][1]),xytext=(0,0),textcoords='offset points')\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        model, index2word_set = init_language_model()\n",
    "\n",
    "        #obtain a list of words from the list of titles + abstracts\n",
    "        no_of_docs = [0 for i in range(k)]\n",
    "        for i in range(k):\n",
    "            documents = []\n",
    "\n",
    "            for j in range(len(preprocessed_list_of_titles)):\n",
    "                if assigned_clusters[j] == i:\n",
    "                    no_of_docs[i] += 1\n",
    "                    documents.append(preprocessed_list_of_titles[j])\n",
    "\n",
    "            bigram = gensim.models.Phrases(documents, min_count=5, threshold=100)\n",
    "            bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "            data_words_bigrams = [bigram_mod[doc] for doc in documents]\n",
    "\n",
    "            # Do lemmatization keeping only noun, adjectives, verbs and adverbs\n",
    "            nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "            data_lemmatized = lemmatization(data_words_bigrams, nlp, ['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "            dictionary = Dictionary(data_lemmatized)\n",
    "            #dictionary = Dictionary(documents)\n",
    "\n",
    "            bow_corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "            lda_model =  gensim.models.LdaMulticore(bow_corpus,\n",
    "                                           num_topics = 1,\n",
    "                                           id2word = dictionary,\n",
    "                                           passes = 20,\n",
    "                                           workers = 2)\n",
    "            #pprint(lda_model.print_topics())\n",
    "            print(lda_model.show_topic(0, topn=10))\n",
    "\n",
    "     #       doc_lda = lda_model[bow_corpus]\n",
    "\n",
    "        for i in range(k):\n",
    "            print(\"-------------------------------- cluster number %d\" %i)\n",
    "            print(\"cluster \" + str(i) + \" contains \" + str(no_of_docs[i]) + \" documents\")\n",
    "            count = 0\n",
    "            for j in range(len(preprocessed_list_of_titles)):\n",
    "                if assigned_clusters[j] == i and count < 10:\n",
    "                    print(\"http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw?REQUEST=GetRecordById&id=\" + str(identifiers[j]) + \"&SERVICE=CSW&VERSION=2.0.2\")\n",
    "                    count += 1\n",
    "                if count >= 10:\n",
    "                    break\n",
    "\n",
    "        split_again = input(\"Do you want to get further categories [Y/n] \")\n",
    "        if split_again == 'n':\n",
    "            break\n",
    "        new_list_of_titles = []\n",
    "        categ = input(\"Which one do you want to get [1/2] \")\n",
    "        for j in range(len(preprocessed_list_of_titles)):\n",
    "            if assigned_clusters[j] == int(categ) - 1:\n",
    "                new_list_of_titles.append(preprocessed_list_of_titles[j])\n",
    "        preprocessed_list_of_titles = new_list_of_titles\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on k-means, you can get the optimal number of clusters based on the previous information\n",
    "#starting from it, use directly the LDA approch to divide articles into topics and determine the most important\n",
    "#words in each topic, using the bag of words approach\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    csw = CatalogueServiceWeb('http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw')\n",
    "    set_title = fes.PropertyIsLike('any', '')#SEARCH_QUERY)\n",
    "    filter_list = [set_title]\n",
    "\n",
    "    csw.getrecords2(constraints=filter_list, maxrecords=2000)\n",
    "\n",
    "    fmt = '{:*^64}'.format\n",
    "    print(fmt(' Catalog information '))\n",
    "    print(\"CSW version: {}\".format(csw.version))\n",
    "    print(\"Number of datasets available: {}\".format(len(csw.records.keys())))\n",
    "    print('\\n')\n",
    "\n",
    "    original_list_of_titles = []\n",
    "    preprocessed_list_of_titles = []\n",
    "    identifiers = []\n",
    "    word2vec_number_list = []\n",
    "    \n",
    "    model, index2word_set = init_language_model()\n",
    "    \n",
    "    for rec in csw.records:\n",
    "        original_list_of_titles.append(csw.records[rec].title)\n",
    "        identifiers.append(csw.records[rec].identifier)\n",
    "        title = prepareDescription(csw.records[rec].title, keepwords)\n",
    "        if csw.records[rec].abstract != '':\n",
    "            abstract = prepareDescription(csw.records[rec].abstract, keepwords)\n",
    "            title = title + \" \" + abstract\n",
    "        \n",
    "        #sent = avg_feature_vector(keyw, model, num_features=NUM_FEATURES, index2word_set=index2word_set)\n",
    "        #if len(title) != 0:\n",
    "        preprocessed_list_of_titles.append(title.split())\n",
    "        #word2vec_number_list.append(sent)\n",
    "\n",
    "    k = 4\n",
    "    documents = []\n",
    "    \n",
    "    #preprocessed_list_of_titles = lemmatization()\n",
    "    \n",
    "    for j in range(len(preprocessed_list_of_titles)):\n",
    "        documents.append(preprocessed_list_of_titles[j])\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    #bigram = gensim.models.Phrases(documents, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    #trigram = gensim.models.Phrases(bigram[documents], threshold=100)\n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    #bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    #trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    #data_words_bigrams = [bigram_mod[doc] for doc in documents]\n",
    "    \n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    tfidf = gensim.models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "    lda_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=k, id2word=dictionary, passes=2, workers=4)\n",
    "    \n",
    "    \"\"\"\n",
    "    lda_model =  gensim.models.LdaMulticore(bow_corpus,\n",
    "                                       num_topics = k,\n",
    "                                       id2word = dictionary,\n",
    "                                       passes = 50,\n",
    "                                       workers = 2)\n",
    "    \"\"\"\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print('Topic: {} Word: {}'.format(idx, topic))\n",
    "    \n",
    "    #pprint(lda_model.show_topics())\n",
    "    \n",
    "    lda_corpus = lda_model[bow_corpus]\n",
    "    scores = list(chain(*[[score for topic_id,score in topic] \\\n",
    "                      for topic in [doc for doc in lda_corpus]]))\n",
    "    threshold = sum(scores)/len(scores)\n",
    "    print(threshold)\n",
    "    print()\n",
    "    \n",
    "    no_of_docs = [0] * k\n",
    "    assigned_clusters = [-1] * len(documents)\n",
    "    \n",
    "    for id in range(k):\n",
    "        no = 0\n",
    "        for i,j in zip(lda_corpus,documents):\n",
    "            #print(str(no) + \"-------------------------------\" + str(i))\n",
    "            if len(i) > id:\n",
    "                if i[id][1] > threshold:\n",
    "                    assigned_clusters[no] = id\n",
    "                    no_of_docs[id] += 1\n",
    "            no += 1\n",
    "    \n",
    "    \n",
    "    for i in range(k):\n",
    "        print(\"cluster number \" + str(i) + \" has \" + str(no_of_docs[i]) +\" entries\")\n",
    "        count = 0\n",
    "        for j in range(len(preprocessed_list_of_titles)):\n",
    "            if assigned_clusters[j] == i and count < 10:\n",
    "                print(\"http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw?REQUEST=GetRecordById&id=\" + str(identifiers[j]) + \"&SERVICE=CSW&VERSION=2.0.2\")\n",
    "                count += 1\n",
    "            if count >= 10:\n",
    "                break\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(k):\n",
    "        print(\"-------------------------------- cluster number %d\" %i)\n",
    "        count = 0\n",
    "        for j in range(len(preprocessed_list_of_titles)):\n",
    "            if assigned_clusters[j] == i and count < 10:\n",
    "                print(\"http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw?REQUEST=GetRecordById&id=\" + str(identifiers[j]) + \"&SERVICE=CSW&VERSION=2.0.2\")\n",
    "                count += 1\n",
    "            if count >= 10:\n",
    "                break\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= try again other techniques ===========\n",
    "\n",
    "#based on k-means, you can get the optimal number of clusters based on the previous information\n",
    "#starting from it, use directly the LDA approch to divide articles into topics and determine the most important\n",
    "#words in each topic, using the bag of words approach\n",
    "\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "\n",
    "def topic_prob_extractor(gensim_hdp):\n",
    "    shown_topics = gensim_hdp.show_topics(num_topics=-1, formatted=False)\n",
    "    topics_nos = [x[0] for x in shown_topics ]\n",
    "    weights = [ sum([item[1] for item in shown_topics[topicN][1]]) for topicN in topics_nos ]\n",
    "\n",
    "    return pd.DataFrame({'topic_id' : topics_nos, 'weight' : weights})\n",
    "\n",
    "def evaluate_graph(dictionary, corpus, texts, limit):\n",
    "    \"\"\"\n",
    "    Function to display num_topics - LDA graph using c_v coherence\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    limit : topic limit\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lm_list : List of LDA topic models\n",
    "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    c_v = []\n",
    "    lm_list = []\n",
    "    for num_topics in range(1, limit):\n",
    "        lm = gensim.models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        lm_list.append(lm)\n",
    "        cm = gensim.models.CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        c_v.append(cm.get_coherence())\n",
    "        \n",
    "    # Show graph\n",
    "    x = range(1, limit)\n",
    "    plt.plot(x, c_v)\n",
    "    plt.xlabel(\"num_topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"c_v\"), loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return lm_list, c_v\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    csw = CatalogueServiceWeb('http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw')\n",
    "    set_title = fes.PropertyIsLike('any', '')#SEARCH_QUERY)\n",
    "    filter_list = [set_title]\n",
    "\n",
    "    csw.getrecords2(constraints=filter_list, maxrecords=2000)\n",
    "\n",
    "    fmt = '{:*^64}'.format\n",
    "    print(fmt(' Catalog information '))\n",
    "    print(\"CSW version: {}\".format(csw.version))\n",
    "    print(\"Number of datasets available: {}\".format(len(csw.records.keys())))\n",
    "    print('\\n')\n",
    "\n",
    "    original_list_of_titles = []\n",
    "    preprocessed_list_of_titles = []\n",
    "    identifiers = []\n",
    "    word2vec_number_list = []\n",
    "    \n",
    "    model, index2word_set = init_language_model()\n",
    "    \n",
    "    for rec in csw.records:\n",
    "        original_list_of_titles.append(csw.records[rec].title)\n",
    "        identifiers.append(csw.records[rec].identifier)\n",
    "        title = prepareDescription(csw.records[rec].title, keepwords)\n",
    "        if csw.records[rec].abstract != '':\n",
    "            abstract = prepareDescription(csw.records[rec].abstract, keepwords)\n",
    "            title = title + \" \" + abstract\n",
    "        \n",
    "        #sent = avg_feature_vector(keyw, model, num_features=NUM_FEATURES, index2word_set=index2word_set)\n",
    "        #if len(title) != 0:\n",
    "        preprocessed_list_of_titles.append(title.split())\n",
    "        #word2vec_number_list.append(sent)\n",
    "\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    preprocessed_list_of_titles = lemmatization(preprocessed_list_of_titles, nlp)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_list_of_titles]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    k = 7\n",
    "    documents = []\n",
    "    \n",
    "    for j in range(len(preprocessed_list_of_titles)):\n",
    "        documents.append(preprocessed_list_of_titles[j])\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(documents, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[documents], threshold=100)\n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    data_words_bigrams = [bigram_mod[doc] for doc in documents]\n",
    "    \n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    \n",
    "    lda_model =  gensim.models.LsiModel(bow_corpus, num_topics = 7,\n",
    "                                       id2word = dictionary)\n",
    "    \n",
    "    print(lda_model.show_topics(num_topics=7))\n",
    "    \"\"\"\n",
    "    \n",
    "    lmlist, c_v = evaluate_graph(dictionary=dictionary, corpus=bow_corpus, texts=preprocessed_list_of_titles, limit=20)\n",
    "    \n",
    "    \"\"\"\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    lda_model =  gensim.models.HdpModel(bow_corpus,\n",
    "                                       id2word = dictionary)\n",
    "    \n",
    "    data_frame = topic_prob_extractor(lda_model)\n",
    "    data_frame = data_frame.sort_values(by='weight', ascending=False)\n",
    "    print(data_frame)\n",
    "\n",
    "\n",
    "    pprint(lda_model.print_topics(-1, 10))\n",
    "    \n",
    "  \n",
    "    lda_corpus = lda_model[bow_corpus]\n",
    "    scores = list(chain(*[[score for topic_id,score in topic] \\\n",
    "                      for topic in [doc for doc in lda_corpus]]))\n",
    "    threshold = sum(scores)/len(scores)\n",
    "    print(threshold)\n",
    "    print()\n",
    "    \n",
    "    k = 150\n",
    "    \n",
    "    for id in range(k):\n",
    "        no = 0\n",
    "        for i,j in zip(lda_corpus,documents):\n",
    "            pprint(i)\n",
    "            if i != []:\n",
    "                if i[id][1] > threshold:\n",
    "                    assigned_clusters[no] = id\n",
    "                no += 1\n",
    "    \n",
    "    for i in range(k):\n",
    "        print(\"-------------------------------- cluster number %d\" %i)\n",
    "        count = 0\n",
    "        for j in range(len(preprocessed_list_of_titles)):\n",
    "            if assigned_clusters[j] == i and count < 10:\n",
    "                print(\"http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw?REQUEST=GetRecordById&id=\" + str(identifiers[j]) + \"&SERVICE=CSW&VERSION=2.0.2\")\n",
    "                count += 1\n",
    "            if count >= 10:\n",
    "                break\n",
    "    \"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on k-means, you can get the optimal number of clusters based on the previous information\n",
    "#starting from it, use directly the LDA approch to divide articles into topics and determine the most important\n",
    "#words in each topic, using the bag of words approach\n",
    "\n",
    "%run \"NLP_clustering.ipynb\"\n",
    "\n",
    "from itertools import chain\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "import operator\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    csw = CatalogueServiceWeb('http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw')\n",
    "    set_title = fes.PropertyIsLike('any', '')#SEARCH_QUERY)\n",
    "    filter_list = [set_title]\n",
    "\n",
    "    csw.getrecords2(constraints=filter_list, maxrecords=2000)\n",
    "\n",
    "    fmt = '{:*^64}'.format\n",
    "    print(fmt(' Catalog information '))\n",
    "    print(\"CSW version: {}\".format(csw.version))\n",
    "    print(\"Number of datasets available: {}\".format(len(csw.records.keys())))\n",
    "    print('\\n')\n",
    "\n",
    "    original_list_of_titles = []\n",
    "    preprocessed_list_of_titles = []\n",
    "    identifiers = []\n",
    "    word2vec_number_list = []\n",
    "    \n",
    "    #model, index2word_set = init_language_model()\n",
    "    \n",
    "    print(\"------------------------------------------------------\")\n",
    "    data = []\n",
    "    for rec in csw.records:\n",
    "        original_list_of_titles.append(csw.records[rec].title)\n",
    "        identifiers.append(csw.records[rec].identifier)\n",
    "        title = prepareDescription(csw.records[rec].title, keepwords)\n",
    "        if csw.records[rec].abstract != '':\n",
    "            abstract = prepareDescription(csw.records[rec].abstract, keepwords)\n",
    "            title = title + \" \" + abstract\n",
    "        \n",
    "        #sent = avg_feature_vector(keyw, model, num_features=NUM_FEATURES, index2word_set=index2word_set)\n",
    "        #if len(title) != 0:\n",
    "        preprocessed_list_of_titles.append(title.split())\n",
    "        data.append(title)\n",
    "        #word2vec_number_list.append(sent)\n",
    "        break\n",
    "   \n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(preprocessed_list_of_titles, min_count=5, threshold=32) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[preprocessed_list_of_titles], threshold=32)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    \n",
    "    print(trigram_mod[bigram_mod[preprocessed_list_of_titles[0]]])\n",
    "    \n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(preprocessed_list_of_titles)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "    k = 5\n",
    "    \n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=0,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=5,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    \n",
    "    pprint(lda_model.print_topics())\n",
    "    doc_lda = lda_model[corpus]\n",
    "    \n",
    "    # Compute Perplexity\n",
    "    print ('Perplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print ('Coherence Score: ', coherence_lda)\n",
    "    \n",
    "    # Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "    mallet_path = 'mallet-2.0.8/bin/mallet' # update this path\n",
    "    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)\n",
    "    \n",
    "    # Show Topics\n",
    "    pprint(ldamallet.show_topics(num_topics=k, formatted=False))\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_ldamallet)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Can take a long time to run.\n",
    "    limit=35; start=2; step=1;\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=id2word,\n",
    "                                                        corpus=corpus,\n",
    "                                                        texts=data_lemmatized,\n",
    "                                                        start=start,\n",
    "                                                        limit=limit,\n",
    "                                                        step=step)\n",
    "    \n",
    "    # Show graph\n",
    "    x = range(start, limit, step)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    # plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the coherence scores\n",
    "    for m, cv in zip(x, coherence_values):\n",
    "        print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 6))\n",
    "        \n",
    "    # Select the model and print the topics\n",
    "    index, value = max(enumerate(coherence_values), key=operator.itemgetter(1))\n",
    "    optimal_model = model_list[index]\n",
    "    k = index + 1\n",
    "    \"\"\"\n",
    "    optimal_model = lda_model\n",
    "    model_topics = optimal_model.show_topics(num_topics=k, formatted=False)\n",
    "    pprint(optimal_model.print_topics(num_words=10))\n",
    "    \n",
    "    optimal_model.show_topic(0,10)\n",
    "    for topic in sorted(optimal_model.show_topics(num_topics=1000, num_words=10, formatted=False), key=lambda x: x[0]):\n",
    "        print('Topic {}: {}'.format(topic[0], [item[0] for item in topic[1]]))\n",
    "\n",
    "        \n",
    "    topics = lda_model.get_document_topics(corpus, per_word_topics=True)\n",
    "    all_topics = [(doc_topics, word_topics, word_phis) for doc_topics, word_topics, word_phis in topics]\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    lda_corpus = lda_model[corpus]\n",
    "    scores = list(chain(*[[score for topic_id,score in topic] \\\n",
    "                      for topic in [doc for doc in lda_corpus]]))\n",
    "    threshold = sum(scores)/len(scores)\n",
    "    print(threshold)\n",
    "    print()\n",
    "    \n",
    "    no_of_docs = [0] * k\n",
    "    assigned_clusters = [-1] * len(documents)\n",
    "    \n",
    "    for id in range(k):\n",
    "        no = 0\n",
    "        for i,j in zip(lda_corpus,documents):\n",
    "            #print(str(no) + \"-------------------------------\" + str(i))\n",
    "            if len(i) > id:\n",
    "                if i[id][1] > threshold:\n",
    "                    assigned_clusters[no] = id\n",
    "                    no_of_docs[id] += 1\n",
    "            no += 1\n",
    "    \n",
    "    \n",
    "    for i in range(k):\n",
    "        print(\"cluster number \" + str(i) + \" has \" + str(no_of_docs[i]) +\" entries\")\n",
    "        count = 0\n",
    "        for j in range(len(preprocessed_list_of_titles)):\n",
    "            if assigned_clusters[j] == i and count < 10:\n",
    "                print(\"http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw?REQUEST=GetRecordById&id=\" + str(identifiers[j]) + \"&SERVICE=CSW&VERSION=2.0.2\")\n",
    "                count += 1\n",
    "            if count >= 10:\n",
    "                break\n",
    "    \n",
    "    \n",
    "    print(\"-----------------------------------------------------\")\n",
    "    for doc in corpus:\n",
    "        topics_distr = lda_model[doc]\n",
    "        print(topics_distr)\n",
    "        for _, topic in enumerate(topics_distr):\n",
    "    \"\"\"\n",
    "    df_topic_sents_keywords = format_topics_sentences(texts=data, ldamodel=optimal_model, corpus=corpus)\n",
    "\n",
    "    # Format\n",
    "    df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    \n",
    "    # Show\n",
    "    #print(df_dominant_topic)\n",
    "    \n",
    "    #print(df_dominant_topic[df_dominant_topic['Dominant_Topic'].isin([0, 1])])\n",
    "    \n",
    "    #print([text.split() for text in df_dominant_topic['Keywords'].tolist()])\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(k):\n",
    "        print(\"--------------------- Topic \" + str(i))\n",
    "        j = 0\n",
    "        count = 0\n",
    "        for idx, row in df_dominant_topic.iterrows():\n",
    "            #print('{}. Dominant keywords: {}'.format(row['Document_No'], row['Keywords'].split(', ')[:5]))\n",
    "            if count >= 10:\n",
    "                break\n",
    "            if int(row['Dominant_Topic']) == i:\n",
    "                print(\"http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw?REQUEST=GetRecordById&id=\" + str(identifiers[j]) + \"&SERVICE=CSW&VERSION=2.0.2\")\n",
    "                count += 1\n",
    "            j += 1\n",
    "    \n",
    "    \"\"\"\n",
    "    # Group top 5 sentences under each topic\n",
    "    sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "    sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "    for i, grp in sent_topics_outdf_grpd:\n",
    "        sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "    # Reset Index    \n",
    "    sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Format\n",
    "    sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "    # Show\n",
    "    print(sent_topics_sorteddf_mallet)\n",
    "\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
