{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classifier \n",
    "# Categorize text with pre-defined labels\n",
    "\n",
    "In case of short texts, as metadata records, the best approach is to build up a hierarchy of pre-defined words\n",
    "related to the topic and assign each text to those categories. \n",
    "\n",
    "The approach in this case is the following:\n",
    "1. run an unsupervised classifier for short texts to obtain several topics\n",
    "2. build a similarity matrix between each set of expert labels and the obtained topics\n",
    "3. add the similarities between each text and its topic to the matrix\n",
    "4. for each topic, arrange the results in descending order, based on the similarity\n",
    "\n",
    "\n",
    "## Unsupervised classification\n",
    "\n",
    "Regarding unsupervised classification, one of the most common techniques is Latent semantic analysis, which creates vector representations of documents. It takes the list of documents as the input corpus and it computes similarities as the distance between vectors. The first step in LSA is to build a term frequency-inverse document frequency (tf-idf) where each position in the vector corresponds to a different word and a document is represented by the number of times each word appears. So, the most important words will be the ones that appear the most often in the documents. In order to make the process better, the LSA algorithms improve the process by also considering synonymity between words.\n",
    "\n",
    "In this case, LSA is not enough for short texts, where the words related to the topic can occur only once or twice in the text. Generally, the technical words are not used often in the same paragraph and they are usually ignored by the LSA algorithm. Even if the stop words are removed. there are still English words in text that occur more often. Even if the unsupervised classifier doesn't bring the best results, it is used as an intermediate step to get the final similarity. Beside the topics, it also returns a matrix of similarity between each document and each topic.\n",
    "\n",
    "Considering:\n",
    "- N = total number of documents in corpus\n",
    "- T1 = total number of automatic topics\n",
    "\n",
    "The results to be kept are the top words for each topic and the matrix of similarity between the documents and the topics of size N x T1.\n",
    "\n",
    "The number of topics is set to a pre-defined number, but the algorithm may find a lower number and return the last topics empty.\n",
    "\n",
    "T = the number of topics obtained as a result, it may be T or less\n",
    "\n",
    " \n",
    "## Build similarity matrix between topics and pre-defined labels\n",
    "\n",
    "Notation:\n",
    "- tw = number of words per topic (set to 30 in this case)\n",
    "- lw = number of words per pre-defined label\n",
    "\n",
    "The next step is to build a classification matrix between the labels and the topics that we obtained at the above step.\n",
    "For each topic, we considered a list of tw words. For each word in label and for each word in topic, we compute the similarity, using the cosine distance of the lanugage model obtained as prerequisite.\n",
    "\n",
    "So, for each topic, we obtain a matrix of size tw x lw, containing the similarities. We save, from each line, the maximum value and we will obtain a vector of tw entries. The final similarity will be computes as the magnitude of the array:\n",
    "w = math.sqrt((tw1)^2 + (tw2)^2 + ... + (twn)^2) / tw\n",
    "\n",
    "\n",
    "## Add the similarities between each text and its topic to the matrix\n",
    "\n",
    "For each document, we have a list of similarity to each automatic topic, meaning an array of length T2 \n",
    "sim_D_T = [sdt1, sdt2, .. sdtT2], where the sum of elements is 1\n",
    "For each label and topic we have a similarity array:\n",
    "sim_T_L = [slt1, slt2, .. sltL]\n",
    "\n",
    "In order to compute the similarity between document and pre-defined label, we apply the following formula:\n",
    "sim_D_L = sim_D_T * sim_T_L\n",
    "\n",
    "Then, the results are analysed per topic. The maximum value is selected and all the values in the corresponding column are divided by it. The entries are then saved in files, in order of relevance, together with the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"Common Defines.ipynb\"\n",
    "%run \"NLP_clustering.ipynb\"\n",
    "%run \"Predefined Labels.ipynb\"\n",
    "\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The files where the similarities can be saved for further testing based on the keywords\n",
    "# These files contain the similarity matrix between each entry in the database and each pre-defined label\n",
    "# and can be then used to get similarities between keywords and documents\n",
    "\n",
    "SIM_MATRIX_FILE = \"geocatalogue_similarity_matrix.csv\"\n",
    "ID_LIST_FILE = \"geocatalogue_id_list.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The language model that will be used\n",
    "# It can be initialized only once and then will be stored in memory for further uses\n",
    "\n",
    "model, index2word_set = init_language_model()\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# The langugage model can be tested on several words to check if it runs correctly\n",
    "print(model.wv.most_similar(\"sky\"))\n",
    "print(model.wv.most_similar(\"downwelling\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_similarity(topic_words_avg, sent_words):\n",
    "    \n",
    "    sent_words_avg = avg_feature_vector((' '.join(sent_words)), model, num_features=NUM_FEATURES, index2word_set=index2word_set)\n",
    "    return 1 - spatial.distance.cosine(sent_words_avg, topic_words_avg)\n",
    "\n",
    "\n",
    "def cosine_similarity_compare (label_words, topic_words):\n",
    "    A = [[0 for x in range(len(label_words))] for y in range(len(topic_words))]\n",
    "    for i in range(len(topic_words)):\n",
    "        tw = topic_words[i]\n",
    "        if tw in model.wv:\n",
    "            for j in range(0, len(label_words)):\n",
    "                lw = label_words[j]\n",
    "                if lw in model.wv:\n",
    "                    A[i][j] = 1 - spatial.distance.cosine(model.wv[tw], model.wv[lw])\n",
    "\n",
    "    maxa = np.max(A, axis = -1, initial = 0)\n",
    "    weight = 0.0\n",
    "\n",
    "    if isinstance(maxa, float):\n",
    "        weight = maxa\n",
    "    else:\n",
    "        for i in range(len(maxa)):\n",
    "            weight += math.pow(maxa[i], 2)\n",
    "\n",
    "        weight = math.sqrt(weight) / len(maxa)\n",
    "\n",
    "    return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " if __name__ == \"__main__\":\n",
    "    csw = CatalogueServiceWeb('http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw')\n",
    "    set_title = fes.PropertyIsLike('any', '')#'solar observations')\n",
    "    filter_list = [set_title]\n",
    "\n",
    "    csw.getrecords2(constraints=filter_list, maxrecords=2000)\n",
    "\n",
    "    fmt = '{:*^64}'.format\n",
    "    print(fmt(' Catalog information '))\n",
    "    print(\"CSW version: {}\".format(csw.version))\n",
    "    print(\"Number of datasets available: {}\".format(len(csw.records.keys())))\n",
    "    print('\\n')\n",
    "\n",
    "    original_list_of_titles = []\n",
    "    identifiers = []\n",
    "    preprocessed_list_of_titles = []\n",
    "\n",
    "    abbr_lower = [abbr.lower() for abbr in list(abbreviations.keys())]\n",
    "\n",
    "    for rec in csw.records:\n",
    "        original_list_of_titles.append(csw.records[rec].title)\n",
    "        identifiers.append(csw.records[rec].identifier)\n",
    "        title = csw.records[rec].title\n",
    "        \n",
    "        title = prepareDescription(title, keepwords, abbr_lower)\n",
    "        title = replace_abbrevations(title, abbreviations)\n",
    "\n",
    "        #remove words duplicates, maybe this will show some better results\n",
    "        title =  ' '.join(list(dict.fromkeys(title.split())))\n",
    "        preprocessed_list_of_titles.append(title.split())\n",
    "\n",
    "\n",
    "    title_sim = [ [0 for i in range(NUM_LABELS)] for j in range(len(preprocessed_list_of_titles))]\n",
    "    title_id = 0\n",
    "    label_words = [time_series_solar_resources, atmosphere_meteorology,\n",
    "                   ground_topography, meteorological_year, solar_potential]\n",
    "    \n",
    "    #compare all the titles to themes, domains and sub_domains\n",
    "    for title in preprocessed_list_of_titles:\n",
    "        domain_sim = 0\n",
    "        \n",
    "        theme_sim = cosine_similarity_compare(themes_title, title)\n",
    "        domain_sim += cosine_similarity_compare(domains_title, title)\n",
    "            \n",
    "        # which of these is the most suitable sub_domain\n",
    "        i = 0\n",
    "        for item in sub_domains:\n",
    "            title_sim[title_id][i] = cosine_similarity_compare(item.split(), title)\n",
    "            i += 1\n",
    "\n",
    "        for i in range(NUM_LABELS):\n",
    "            title_sim[title_id][i] += cosine_similarity_compare(label_words[i], title)\n",
    "\n",
    "        for i in range(NUM_LABELS):\n",
    "            title_sim[title_id][i] += cosine_similarity_compare(info[i].split(), title)\n",
    "            \n",
    "        for i in range(NUM_LABELS):\n",
    "            title_sim[title_id][i] = (title_sim[title_id][i]) / 3# + domain_sim + theme_sim) / 5\n",
    "\n",
    "        title_id += 1\n",
    "\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(title_sim)\n",
    "    print(\"-------------------------------------------\")\n",
    "\n",
    "    # add abstract and keywords for comparisons\n",
    "    preprocessed_list_of_titles = []\n",
    "\n",
    "    for rec in csw.records:\n",
    "        title = csw.records[rec].title\n",
    "        \n",
    "        if csw.records[rec].abstract != None:\n",
    "            title = title + \" \" + csw.records[rec].abstract\n",
    "\n",
    "        if len(csw.records[rec].subjects) > 0 and csw.records[rec].subjects != [None]:\n",
    "            keywords_set = build_keywords_set(csw.records[rec].subjects, [])\n",
    "            if len(keywords_set) > 0:\n",
    "                title = title + ' '.join(keywords_set)\n",
    "\n",
    "        title = prepareDescription(title, keepwords, abbr_lower)\n",
    "        title = replace_abbrevations(title, abbreviations)\n",
    "\n",
    "        #remove words duplicates, maybe this will show some better results\n",
    "        title =  ' '.join(list(dict.fromkeys(title.split())))\n",
    "        preprocessed_list_of_titles.append(title.split())\n",
    "\n",
    "\n",
    "    T=10\n",
    "    mgp = MovieGroupProcess(K=T, alpha=0.1, beta=0.1, n_iters=30)\n",
    "    vocab = set(x for doc in preprocessed_list_of_titles for x in doc)\n",
    "    n_terms = len(vocab)\n",
    "    y = mgp.fit(preprocessed_list_of_titles, n_terms)\n",
    "    \n",
    "    \n",
    "    # Save model\n",
    "    with open(\"sttm_v1.model\", \"wb\") as f:\n",
    "        pickle.dump(mgp, f)\n",
    "        f.close()\n",
    "    \n",
    "    doc_count = np.array(mgp.cluster_doc_count)\n",
    "    print('Number of documents per topic :', doc_count)\n",
    "    print('*'*20)# Topics sorted by the number of document they are allocated to\n",
    "    top_index = doc_count.argsort()[(-1*T):][::-1]\n",
    "    \n",
    "    print('*'*20)# Show the top 5 words in term frequency for each cluster\n",
    "    \n",
    "    \n",
    "    label_words_measures = [time_series_solar_resources_measures, atmosphere_meteorology_measures,\n",
    "                   ground_topography_measures, meteorological_year_measures, solar_potential_measures]\n",
    "    \n",
    "    sims_T_L = [ [0 for i in range(NUM_LABELS)] for j in range(len(mgp.cluster_word_distribution))]\n",
    "    \n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    topics = []\n",
    "    \n",
    "    for cluster_dict_per_topic in mgp.cluster_word_distribution:\n",
    "        counter = Counter(cluster_dict_per_topic)\n",
    "\n",
    "        high = counter.most_common(30)\n",
    "\n",
    "        if high == []:\n",
    "            continue\n",
    "\n",
    "        topic_words = [x[0] for x in high]\n",
    "        # the most common words, how are they connected to each predefined topic?\n",
    "        for i in range(len(label_words)):\n",
    "            sims_T_L[count][i] = cosine_similarity_compare(label_words[i], topic_words)\n",
    "\n",
    "        topics.extend(topic_words)\n",
    "        count += 1\n",
    "    \n",
    "    T2 = count\n",
    "    \n",
    "    count = 0\n",
    "    sims_D_T = [ [0 for i in range(T2)] for j in range(len(preprocessed_list_of_titles))]\n",
    "    \n",
    "    for doc in preprocessed_list_of_titles:\n",
    "        sims_D_T[count] = mgp.score(doc)\n",
    "        count += 1\n",
    "\n",
    "    print(\"----------------------------------se inmulteste cu:\")\n",
    "    print(sims_T_L)\n",
    "    print(\"----------------------------------\")\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"----------------------------------se inmulteste cu:\")\n",
    "    print(sims_D_T)\n",
    "    print(\"----------------------------------\")\n",
    "    \"\"\"\n",
    "\n",
    "    # multiply matrices\n",
    "    sims_D_T = np.array(sims_D_T)\n",
    "    sims_T_L = np.array(sims_T_L)\n",
    "    sims_D_L = np.zeros([np.size(sims_D_T, 0), np.size(sims_T_L, 1)])\n",
    "    \n",
    "    for i in range(np.size(sims_D_T, 0)):\n",
    "        for j in range(np.size(sims_T_L, 1)):\n",
    "            if np.count_nonzero(sims_D_T[i, :]) > 0 and np.count_nonzero(sims_T_L[:, j]) > 0:\n",
    "                sims_D_L[i][j] = np.matmul(sims_D_T[i, :], sims_T_L[:, j])\n",
    "                #spatial.distance.cosine(sims_D_T[i, :], sims_T_L[:, j])\n",
    "            else:\n",
    "                sims_D_L[i][j] = 0\n",
    "\n",
    "    no_lines = len(sims_D_L)\n",
    "    no_cols = len(sims_D_L[0])\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"----------------------------------rezultatul e:\")\n",
    "    print(sims_D_L)\n",
    "    print(\"----------------------------------\")\n",
    "    \"\"\"\n",
    "\n",
    "    #which are the words from the concepts which are not considered by the automatic features\n",
    "    topics = list(dict.fromkeys(topics))\n",
    "    print(\"set of topics: \", topics)\n",
    "    \n",
    "    sim_labels = []\n",
    "    i = 0\n",
    "    for label in label_words:\n",
    "        label_list = [w for w in label if w not in topics]\n",
    "        sim_labels.append(label_list)\n",
    "        i += 1\n",
    "\n",
    "    print(\"sim_labels:\")\n",
    "    print(sim_labels)\n",
    "    \n",
    "    i = 0\n",
    "    sims_D_L_2 = [ [0 for i in range(NUM_LABELS)] for j in range(len(preprocessed_list_of_titles))]\n",
    "\n",
    "    # compare these to the corpus and gather similarities\n",
    "    for count in range(len(preprocessed_list_of_titles)):\n",
    "        sentence  = preprocessed_list_of_titles[count]\n",
    "        if sentence != []:\n",
    "            for i in range(len(sim_labels)):\n",
    "                label = sim_labels[i]\n",
    "                sims_D_L_2[count][i] = cosine_similarity_compare(label, sentence)\n",
    "\n",
    "    i = 0\n",
    "    sims_D_L_3 = [ [0 for i in range(NUM_LABELS)] for j in range(len(preprocessed_list_of_titles))]\n",
    "\n",
    "    # compare the measures labels to the corpus and gather similarities\n",
    "    for count in range(len(preprocessed_list_of_titles)):\n",
    "        sentence  = preprocessed_list_of_titles[count]\n",
    "        if sentence != []:\n",
    "            for i in range(len(label_words_measures)):\n",
    "                label = label_words_measures[i]\n",
    "                sims_D_L_3[count][i] = cosine_similarity_compare(label, sentence)\n",
    "    \n",
    "    # how many lines and columns for sim_D_L_2\n",
    "    sims_D_L_2_lines = len(sims_D_L_2)\n",
    "    sims_D_L_2_cols = len(sims_D_L_2[0])\n",
    "\n",
    "    \"\"\"\n",
    "    A = np.matrix.flatten(np.matrix(sims_D_L_2))\n",
    "    A = np.mat(A)\n",
    "    A = preprocessing.normalize(A)\n",
    "    sims_D_L_2 = A.reshape(sims_D_L_2_lines, sims_D_L_2_cols)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"prima matrice: \")\n",
    "    print(sims_D_L)\n",
    "    \n",
    "    print(\"a doua matrice: \")\n",
    "    print(sims_D_L_2)\n",
    "\n",
    "\n",
    "    for i in range(sims_D_L_2_lines):\n",
    "        for j in range(sims_D_L_2_cols):\n",
    "            sims_D_L[i][j] = (title_sim[i][j] * 3 + sims_D_L[i][j] * 3 + sims_D_L_2[i][j] * 2 + sims_D_L_3[i][j] * 2) / 10\n",
    "            if sims_D_L[i][j] > 1:\n",
    "                print(\"exista valori mai mari ca 1: sims_D_L[i][j]\")\n",
    "\n",
    "    for i in range(len(sims_D_L)):\n",
    "        maxv = max(sims_D_L[i])\n",
    "        sims_D_L[i] = [x / maxv for x in np.array(sims_D_L[i])]\n",
    "    \n",
    "    with open(SIM_MATRIX_FILE, \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            [writer.writerow(r) for r in sims_D_L]\n",
    "    \n",
    "    with open(ID_LIST_FILE, \"w\") as f:\n",
    "        for idname in identifiers:\n",
    "            f.write(idname + \"\\n\")\n",
    "    \n",
    "\n",
    "    print(\"----------------------- final matrix\")\n",
    "    print(sims_D_L)\n",
    "    print(\"-----------------------------------------\")\n",
    "    sim_values_trans = np.array(sims_D_L).transpose()\n",
    "\n",
    "    # print best resources for each topic\n",
    "    cnt = 0\n",
    "    for line in sim_values_trans:\n",
    "        arr = np.array(line)\n",
    "        idx = arr.argsort()[-len(identifiers):][::-1]\n",
    "        count = 0\n",
    "        with open(\"results_file\" + str(cnt) + \".txt\", \"w\") as f:\n",
    "            for i in idx:\n",
    "                count += 1\n",
    "                if (count > 10):\n",
    "                    break\n",
    "\n",
    "                if arr[i] > 0:\n",
    "                    linktext = \"http://geocatalog.webservice-energy.org/geonetwork/srv/eng/main.search.embedded?any=\" + str(identifiers[i]) + \"&dummyfield=&northBL=&westBL=&eastBL=&southBL=&relation=overlaps&region_simple=&sortBy=relevance&sortOrder=&hitsPerPage=10&output=full\\n\"\n",
    "                    resource = urllib.request.urlopen(linktext)\n",
    "                    content = resource.read().decode(resource.headers.get_content_charset())\n",
    "\n",
    "                    if \"metadata.show\" in content:\n",
    "                        m = re.search('metadata.show\\?id=([0-9]+)', content)\n",
    "                        if m:\n",
    "                            found = m.group(1)\n",
    "                            linktext = \"http://geocatalog.webservice-energy.org/geonetwork/srv/eng/metadata.show?id=\" + found + \"&currTab=simple\"\n",
    "                            f.write(str(arr[i]) + \" \" + linktext + \"\\n\")\n",
    "                            #print(str(arr[i]) + \" \" + linktext)\n",
    "                        else:\n",
    "                            linktext = \"http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw?REQUEST=GetRecordById&id=\" + str(identifiers[i]) + \"&SERVICE=CSW&VERSION=2.0.2\"\n",
    "                            f.write(str(arr[i]) + \" \" + linktext + \"\\n\")\n",
    "                            #print(str(arr[i]) + \" \" + linktext)\n",
    "\n",
    "        cnt += 1\n",
    "        print('*'*44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Open the CSV file\n",
    "# and compute similarity <label> - <query>\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(SIM_MATRIX_FILE, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        similarity_matrix =  [[float(e) for e in r] for r in reader]\n",
    "        \n",
    "    search_query = \"solar pond\"\n",
    "    \n",
    "    csw = CatalogueServiceWeb('http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw')\n",
    "    set_title = fes.PropertyIsLike('any', search_query)\n",
    "    filter_list = [set_title]\n",
    "\n",
    "    csw.getrecords2(constraints=filter_list, maxrecords=2000)\n",
    "\n",
    "    fmt = '{:*^64}'.format\n",
    "    print(fmt(' Catalog information '))\n",
    "    print(\"CSW version: {}\".format(csw.version))\n",
    "    print(\"Number of datasets available: {}\".format(len(csw.records.keys())))\n",
    "    print('\\n')\n",
    "\n",
    "    original_list_of_titles = []\n",
    "    identifiers = []\n",
    "    preprocessed_list_of_titles = []\n",
    "\n",
    "    abbr_lower = [abbr.lower() for abbr in list(abbreviations.keys())]\n",
    "\n",
    "    for rec in csw.records:\n",
    "        original_list_of_titles.append(csw.records[rec].title)\n",
    "        identifiers.append(csw.records[rec].identifier)\n",
    "        title = csw.records[rec].title\n",
    "        \n",
    "        if csw.records[rec].abstract != None:\n",
    "            title = title + \" \" + csw.records[rec].abstract\n",
    "\n",
    "        if len(csw.records[rec].subjects) > 0 and csw.records[rec].subjects != [None]:\n",
    "            keywords_set = build_keywords_set(csw.records[rec].subjects, [])\n",
    "            if len(keywords_set) > 0:\n",
    "                title = title + ' '.join(keywords_set)\n",
    "\n",
    "        title = prepareDescription(title, keepwords, abbr_lower)\n",
    "        title = replace_abbrevations(title, abbreviations)\n",
    "        \n",
    "        #remove words duplicates, maybe this will show some better results\n",
    "        title =  ' '.join(list(dict.fromkeys(title.split())))\n",
    "        preprocessed_list_of_titles.append(title.split())\n",
    "\n",
    "    # read the matrix from the file and get the similarities\n",
    "    with open(SIM_MATRIX_FILE, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        similarity_matrix = [[float(e) for e in r] for r in reader]\n",
    "\n",
    "    all_identifiers = [line.strip() for line in open(ID_LIST_FILE, 'r')]\n",
    "    \n",
    "    # build similarity matrix <query> <label>\n",
    "    \n",
    "    sims = [ [0 for i in range(NUM_LABELS)] for j in range(len(mgp.cluster_word_distribution))]\n",
    "\n",
    "    label_words = [time_series_solar_resources, atmosphere_meteorology,\n",
    "                   ground_topography, meteorological_year, solar_potential_measures]\n",
    "\n",
    "    query_topic_sims = []\n",
    "    for label in label_words:\n",
    "        query_topic_sims.append(cosine_similarity_compare(label, search_query.split()))\n",
    "    \n",
    "    print(\"???????????\")\n",
    "    print(query_topic_sims)\n",
    "    print(\"???????????\")\n",
    "        \n",
    "    print(\"topic sims: \", query_topic_sims)\n",
    "    \n",
    "    sim_tuples = []\n",
    "    \n",
    "    for id_res in all_identifiers:\n",
    "        if not id_res in identifiers:\n",
    "            # if it's not in in the result list, check the similarity between the article and each topic\n",
    "            index = all_identifiers.index(id_res)\n",
    "            sims = similarity_matrix[index]\n",
    "            \n",
    "            sim_tuples.append((id_res, np.dot(np.matrix(query_topic_sims), np.matrix(sims).T).item(0, 0)))\n",
    "    \n",
    "    \n",
    "    sim_tuples.sort(key=itemgetter(1), reverse = True)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for idx, value in sim_tuples:\n",
    "        linktext = \"http://geocatalog.webservice-energy.org/geonetwork/srv/eng/main.search.embedded?any=\" + idx + \"&dummyfield=&northBL=&westBL=&eastBL=&southBL=&relation=overlaps&region_simple=&sortBy=relevance&sortOrder=&hitsPerPage=10&output=full\\n\"\n",
    "        resource = urllib.request.urlopen(linktext)\n",
    "        content = resource.read().decode(resource.headers.get_content_charset())\n",
    "        if \"metadata.show\" in content:\n",
    "            m = re.search('metadata.show\\?id=([0-9]+)', content)\n",
    "            if m:\n",
    "                found = m.group(1)\n",
    "                linktext = \"http://geocatalog.webservice-energy.org/geonetwork/srv/eng/metadata.show?id=\" + found + \"&currTab=simple\"\n",
    "                \n",
    "            else:\n",
    "                linktext = \"http://geocatalog.webservice-energy.org/geonetwork/srv/eng/csw?REQUEST=GetRecordById&id=\" + str(identifiers[i]) + \"&SERVICE=CSW&VERSION=2.0.2\"\n",
    "                #f.write(str(arr[i]) + \" \" + linktext + \"\\n\")\n",
    "    \n",
    "            print(value, ' ', linktext)\n",
    "        if count == 10:\n",
    "            break\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"Utils_Zenodo.ipynb\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    documents = []\n",
    "    search_query = \"solar irradiance\"\n",
    "    \n",
    "    results = get_zenodo_entries(search_query)\n",
    "    \n",
    "    # the whole list of documents - consider this our database\n",
    "    #documents.extend(get_zenodo_entries(\"photovoltaic\"))\n",
    "    time.sleep(2.4)\n",
    "    documents.extend(get_zenodo_entries(\"Renewable Energy\"))\n",
    "    time.sleep(2.4)\n",
    "    documents.extend(get_zenodo_entries(\"solar pond\"))\n",
    "    time.sleep(2.4)\n",
    "    documents.extend(get_zenodo_entries(\"solar observations\"))\n",
    "     \n",
    "    #for each document in the database. check if it in the list of results\n",
    "    #otherwise, apply the algorithm\n",
    "    search_doi = [article['doi'] for article in results]\n",
    "    database_doi = [article['doi'] for article in documents]\n",
    "    database_doi = [doi for doi in database_doi if doi not in search_doi]\n",
    "\n",
    "    # we already have the fies downloaded\n",
    "    # so look for them in the /tmp folder\n",
    "    # otherwise, download it\n",
    "    for doc in documents:\n",
    "        if doc['doi'] in database_doi:\n",
    "            mod_doi = doc['doi'].replace('/', '-')\n",
    "\n",
    "            #look in tmp folder if there is a file containing the doi\n",
    "            # if there is, just read the file and move to the next entry\n",
    "            if os.path.isfile('/tmp/' + mod_doi + \".txt\"):\n",
    "                continue\n",
    "\n",
    "            doc_list = save_pdf_and_get_text(doc['files'][0]['links']['self'])\n",
    "            # overwrite the doi temporary file\n",
    "            with open('/tmp/' + mod_doi + \".txt\", 'w') as f:\n",
    "                f.write(' '.join(doc_list))\n",
    "                f.close()\n",
    "\n",
    "    #get each file in the folder and build a corpus based on those articles\n",
    "    corpus = []\n",
    "    doi_list = []\n",
    "    for doi in database_doi:\n",
    "        mod_doi = doi.replace('/', '-')\n",
    "        with open('/tmp/' + mod_doi + \".txt\", 'r') as f:\n",
    "            corpus.append(f.read())\n",
    "            doi_list.append(doi)\n",
    "    print(corpus)\n",
    "\n",
    "    NUM_LABELS = 5\n",
    "    K=10\n",
    "\n",
    "    mgp = MovieGroupProcess(K=K, alpha=0.1, beta=0.1, n_iters=30)\n",
    "    vocab = set(x for doc in corpus for x in doc)\n",
    "    n_terms = len(vocab)\n",
    "    y = mgp.fit(corpus, n_terms)\n",
    "        \n",
    "    # Save model\n",
    "    with open(\"sttm_v1.model\", \"wb\") as f:\n",
    "        pickle.dump(mgp, f)\n",
    "        f.close()\n",
    "    \n",
    "\n",
    "    sims = [ [0 for i in range(NUM_LABELS)] for j in range(len(mgp.cluster_word_distribution))]\n",
    "    label_words = [time_series_solar_resources, atmosphere_meteorology,\n",
    "                   ground_topography, meteorological_year, solar_potential_measures]\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for cluster_dict_per_topic in mgp.cluster_word_distribution:\n",
    "        counter = Counter(cluster_dict_per_topic)\n",
    "\n",
    "        high = counter.most_common(30)\n",
    "\n",
    "        print('!'*20)\n",
    "        print(high)\n",
    "        print('!'*20)\n",
    "\n",
    "        # the most common words, how are they connected to each predefined topic?\n",
    "        for i in range(len(label_words)):\n",
    "            sims[count][i] = cosine_similarity_compare(label_words[i], [x[0] for x in high])\n",
    "\n",
    "        print('*'*20)\n",
    "        print(sims[count])\n",
    "        print('*'*20)\n",
    "        count += 1\n",
    "\n",
    "    count = 0\n",
    "    similarity_values = [ [0 for i in range(NUM_LABELS)] for j in range(len(corpus))]\n",
    "    for doc in preprocessed_list_of_titles:\n",
    "        for scoreid in range(len(mgp.score(doc))):\n",
    "            score = mgp.score(doc)[scoreid]\n",
    "            if score > 0.1:\n",
    "                for i in range(NUM_LABELS):\n",
    "                    similarity_values[count][i] += score * sims[scoreid][i]\n",
    "                # there is only one topic\n",
    "                topic_index = mgp.score(doc)\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    similarity_values_cols = [ [0.0 for i in range(no_cols)] for j in range(no_lines)]\n",
    "    #compute maximum per entry and divide everything by it\n",
    "    for col in range(0, len(similarity_values[0])):\n",
    "        maxv = similarity_values[0][col]\n",
    "        for line in range(1, len(similarity_values)):\n",
    "            if similarity_values[line][col] > maxv:\n",
    "                maxv = similarity_values[line][col]\n",
    "        if maxv > 0.0:\n",
    "            for line in range(len(similarity_values)):\n",
    "                similarity_values_cols[line][col] = similarity_values[line][col] / maxv\n",
    "\n",
    "\n",
    "    sim_values_trans = np.array(similarity_values_cols).transpose()\n",
    "    \n",
    "    # print best resources for each topic\n",
    "    for line in sim_values_trans:\n",
    "        cnt = 0\n",
    "        arr = np.array(line)\n",
    "        idx = arr.argsort()[-len(identifiers):][::-1]\n",
    "        for i in idx:\n",
    "            if arr[i] > 0:\n",
    "                linktext = \"doi.org/\" + doi_list[i]\n",
    "                print(arr[i], \" \", linktest)\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt == 20:\n",
    "                break\n",
    "        print('*'*20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
