{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a language model for a specific subject\n",
    "\n",
    "\n",
    "One of the state-of-the art solutions to build NLP application is using word embeddings to\n",
    "compute similarities between texts. Generally, they are vector representations of words that\n",
    "are capable of capturing the context of a word in a document or relation with other words.\n",
    "\n",
    "Word2vec is a two-layer neural network that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. The purpose and usefulness of Word2vec is to group the vectors of similar words together in vectorspace. This way. it is able to detect similarities mathematically [1].\n",
    "\n",
    "A pre-trained set of vectors containing part of Google News dataset can be downloaded and used [2].\n",
    "In case of technical fields, however, in order to achieve a better accuracy, a new language model should be trained,\n",
    "in order to obtain the specific vocabulary that may be missing from the news dataset.\n",
    "\n",
    "## Build up an input corpus\n",
    "\n",
    "The first step is to build a corpus of specialized words, that can be used to get the vocabulary.\n",
    "In order to do that, we decided to use papers stored in Zenodo [3], since it offers a free end point for\n",
    "querying. In order to do that, we raise multiple queries to the server and we store the results locally.\n",
    "The approach that we selected was to download the pdf files and transform them to txt, using a command lines tool\n",
    "for Linux. To speed up the process and to avoid duplicates, we save the files in the '/tmp' folder and check, each time, if the file is already there. In case it is, we load the text, which is already pre-processed. I case of builing a language model, it is better to not have duplicates, since it can change the values attached to words.\n",
    "\n",
    "The next step is to cleanup the text. In order to to that, the first step is to remove all punctation and replace non-alpha characters to spaces. Them all the words are transformed to lowercase and lemmatized to their basis word. Then,\n",
    "all the stop words are removed, together with the non-English and short words. In this case, it is important to keep the  abbreviations, technical words which are not the English dictionary and short words related to the concerned field.\n",
    "Them all the abreviations are replaced by the corresponding words sequence, to be able to compute similarities right. The clean-up texts are then saved in the corresponding text files in the '/tmp' folder.\n",
    "\n",
    "## Build a language model\n",
    "\n",
    "The corpus of documents is directly sent to word2vec library of gensim [4] and then saved in a binary file for further\n",
    "use.\n",
    "\n",
    "\n",
    "[1] https://skymind.ai/wiki/word2vec\n",
    "[2] https://code.google.com/archive/p/word2vec/\n",
    "[3] https://zenodo.org/\n",
    "[4] https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"NLP_clustering.ipynb\"\n",
    "%run \"Utils_Zenodo.ipynb\"\n",
    "\n",
    "\n",
    "def get_entries_from_zenodo(query):\n",
    "    \n",
    "    total_articles = get_zenodo_entries(query)\n",
    "    documents = []\n",
    "    \n",
    "    for article in total_articles:\n",
    "        doi = article['doi']\n",
    "        mod_doi = doi.replace('/', '-')\n",
    "\n",
    "        #look in tmp folder if there is a file containing the doi\n",
    "        # if there is, just read the file and move to the next entry\n",
    "        if os.path.isfile('/tmp/' + doi + \".txt\"):\n",
    "            with open('/tmp/' + doi + \".txt\") as f:\n",
    "                doc_list = f.read().split(' ')\n",
    "        else:\n",
    "            print(article['files'][0]['links']['self'])\n",
    "            # download the file and preprocess in the information\n",
    "            doc_list = save_pdf_and_get_text(article['files'][0]['links']['self'])\n",
    "            # overwrite the doi temporary file\n",
    "            with open('/tmp/' + mod_doi + \".txt\", 'w') as f:\n",
    "                f.write(' '.join(doc_list))\n",
    "                f.close()\n",
    "            # add the text to the corpus   \n",
    "            if doc_list:\n",
    "                documents.append(doc_list)\n",
    " \n",
    "    return documents\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #documents is a list of lists\n",
    "    documents = get_entries_from_zenodo(SEARCH_QUERY)\n",
    "    documents.extend(get_entries_from_zenodo(\"solar irradiance\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"photovoltaic\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"Renewable Energy\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"solar pond\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"solar observations\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"global horizontal irradiance\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"irradiance\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"solar collector\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"downwelling longwave\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"clear sky\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"diffuse radiation\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"water vapor\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"snowdepth\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"bi-directional reflectance\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"snowcover\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"airmass\"))\n",
    "    documents.extend(get_entries_from_zenodo(\"downwelling\"))\n",
    "    \n",
    "    model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "    model.train(documents,total_examples=len(documents),epochs=10)\n",
    "    model.wv.save_word2vec_format(MODEL_TO_TRAIN, binary=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.wv.vocab))\n",
    "#print(documents)\n",
    "\n",
    "#print(model.wv.most_similar('solar'))\n",
    "#print(model.wv.most_similar('photovoltaic'))\n",
    "\n",
    "print(model.wv.most_similar('irradiance'))\n",
    "print(model.wv.most_similar('solar'))\n",
    "print(model.wv.most_similar('photovoltaic'))\n",
    "\n",
    "model.wv.most_similar(\"sky\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.wv.vocab))\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
