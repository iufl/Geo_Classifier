{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classifier \n",
    "# Categorize text with pre-defined labels\n",
    "\n",
    "In case of short texts, as metadata records, the best approach is to build up a hierarchy of pre-defined words\n",
    "related to the topic and assign each text to those categories. \n",
    "\n",
    "The approach in this case is the following:\n",
    "1. run an unsupervised classifier for short texts to obtain several topics\n",
    "2. build a similarity matrix between each set of expert labels and the obtained topics\n",
    "3. add the similarities between each text and its topic to the matrix\n",
    "4. for each topic, arrange the results in descending order, based on the similarity\n",
    "\n",
    "\n",
    "## Unsupervised classification\n",
    "\n",
    "Regarding unsupervised classification, one of the most common techniques is Latent semantic analysis, which creates vector representations of documents. It takes the list of documents as the input corpus and it computes similarities as the distance between vectors. The first step in LSA is to build a term frequency-inverse document frequency (tf-idf) where each position in the vector corresponds to a different word and a document is represented by the number of times each word appears. So, the most important words will be the ones that appear the most often in the documents. In order to make the process better, the LSA algorithms improve the process by also considering synonymity between words.\n",
    "\n",
    "In this case, LSA is not enough for short texts, where the words related to the topic can occur only once or twice in the text. Generally, the technical words are not used often in the same paragraph and they are usually ignored by the LSA algorithm. Even if the stop words are removed. there are still English words in text that occur more often. Even if the unsupervised classifier doesn't bring the best results, it is used as an intermediate step to get the final similarity. Beside the topics, it also returns a matrix of similarity between each document and each topic.\n",
    "\n",
    "Considering:\n",
    "- N = total number of documents in corpus\n",
    "- T1 = total number of automatic topics\n",
    "\n",
    "The results to be kept are the top words for each topic and the matrix of similarity between the documents and the topics of size N x T1.\n",
    "\n",
    "The number of topics is set to a pre-defined number, but the algorithm may find a lower number and return the last topics empty.\n",
    "\n",
    "T = the number of topics obtained as a result, it may be T or less\n",
    "\n",
    " \n",
    "## Build similarity matrix between topics and pre-defined labels\n",
    "\n",
    "Notation:\n",
    "- tw = number of words per topic (set to 30 in this case)\n",
    "- lw = number of words per pre-defined label\n",
    "\n",
    "The next step is to build a classification matrix between the labels and the topics that we obtained at the above step.\n",
    "For each topic, we considered a list of tw words. For each word in label and for each word in topic, we compute the similarity, using the cosine distance of the lanugage model obtained as prerequisite.\n",
    "\n",
    "So, for each topic, we obtain a matrix of size tw x lw, containing the similarities. We save, from each line, the maximum value and we will obtain a vector of tw entries. The final similarity will be computes as the magnitude of the array:\n",
    "w = math.sqrt((tw1)^2 + (tw2)^2 + ... + (twn)^2) / tw\n",
    "\n",
    "\n",
    "## Add the similarities between each text and its topic to the matrix\n",
    "\n",
    "For each document, we have a list of similarity to each automatic topic, meaning an array of length T2 \n",
    "sim_D_T = [sdt1, sdt2, .. sdtT2], where the sum of elements is 1\n",
    "For each label and topic we have a similarity array:\n",
    "sim_T_L = [slt1, slt2, .. sltL]\n",
    "\n",
    "In order to compute the similarity between document and pre-defined label, we apply the following formula:\n",
    "sim_D_L = sim_D_T * sim_T_L\n",
    "\n",
    "Then, the results are analysed per topic. The maximum value is selected and all the values in the corresponding column are divided by it. The entries are then saved in files, in order of relevance, together with the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'__imports__.ipynb.py'` not found.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LANGUAGE_MODEL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mE:\\Geo_Classifier\\NLP_clustering.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0mto\u001b[0m \u001b[0mget\u001b[0m \u001b[0msimilarities\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m '''\n\u001b[1;32m---> 67\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0minit_language_model\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLANGUAGE_MODEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LANGUAGE_MODEL' is not defined"
     ]
    }
   ],
   "source": [
    "%run \"Common Defines Biologic Process.ipynb\"\n",
    "%run \"NLP_clustering.ipynb\"\n",
    "%run \"Predefined Labels - Biologic Process.ipynb\"\n",
    "\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The files where the similarities can be saved for further testing based on the keywords\n",
    "# These files contain the similarity matrix between each entry in the database and each pre-defined label\n",
    "# and can be then used to get similarities between keywords and documents\n",
    "\n",
    "SIM_MATRIX_FILE = \"biologic_process_similarity_matrix.csv\"\n",
    "ID_LIST_FILE = \"biologic_process_id_list.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biogenesis.bin\n",
      "[('tissue', 0.9093469381332397), ('human', 0.8972060680389404), ('vessel', 0.8961011171340942), ('cellular', 0.8946257829666138), ('protein', 0.8868533372879028), ('unraveled', 0.8854694366455078), ('expression', 0.8852810859680176), ('gene', 0.8829666376113892), ('molecular', 0.877745509147644), ('signal', 0.8768489360809326)]\n",
      "[('proliferation', 0.9623371362686157), ('differentiation', 0.9575070142745972), ('unraveled', 0.932964563369751), ('expression', 0.9228825569152832), ('cellular', 0.9145495295524597), ('embryonic', 0.9032720923423767), ('migration', 0.9003199338912964), ('receptive', 0.8990265727043152), ('endothelial', 0.8990079164505005), ('response', 0.8969208598136902)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iulia-MariaFLOREA\\anaconda3\\envs\\geoss_env_win\\lib\\site-packages\\ipykernel_launcher.py:70: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "C:\\Users\\Iulia-MariaFLOREA\\anaconda3\\envs\\geoss_env_win\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Iulia-MariaFLOREA\\anaconda3\\envs\\geoss_env_win\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# The language model that will be used\n",
    "# It can be initialized only once and then will be stored in memory for further uses\n",
    "\n",
    "model, index2word_set = init_language_model(\"biogenesis.bin\")\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# The langugage model can be tested on several words to check if it runs correctly\n",
    "print(model.wv.most_similar(\"cell\"))\n",
    "print(model.wv.most_similar(\"adhesion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_similarity(topic_words_avg, sent_words):\n",
    "    \n",
    "    sent_words_avg = avg_feature_vector((' '.join(sent_words)), model, num_features=NUM_FEATURES, index2word_set=index2word_set)\n",
    "    return 1 - spatial.distance.cosine(sent_words_avg, topic_words_avg)\n",
    "\n",
    "\n",
    "def cosine_similarity_compare (label_words, topic_words):\n",
    "    A = [[0 for x in range(len(label_words))] for y in range(len(topic_words))]\n",
    "    for i in range(len(topic_words)):\n",
    "        tw = topic_words[i]\n",
    "        if tw in model.wv:\n",
    "            for j in range(0, len(label_words)):\n",
    "                lw = label_words[j]\n",
    "                if lw in model.wv:\n",
    "                    A[i][j] = 1 - spatial.distance.cosine(model.wv[tw], model.wv[lw])\n",
    "\n",
    "    maxa = np.max(A, axis = -1, initial = 0)\n",
    "    weight = 0.0\n",
    "\n",
    "    if isinstance(maxa, float):\n",
    "        weight = maxa\n",
    "    else:\n",
    "        for i in range(len(maxa)):\n",
    "            weight += math.pow(maxa[i], 2)\n",
    "\n",
    "        weight = math.sqrt(weight) / len(maxa)\n",
    "\n",
    "    return weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iulia-MariaFLOREA\\anaconda3\\envs\\geoss_env_win\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Iulia-MariaFLOREA\\anaconda3\\envs\\geoss_env_win\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\Users\\Iulia-MariaFLOREA\\anaconda3\\envs\\geoss_env_win\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 752 clusters with 10 clusters populated\n",
      "In stage 1: transferred 423 clusters with 10 clusters populated\n",
      "In stage 2: transferred 185 clusters with 10 clusters populated\n",
      "In stage 3: transferred 92 clusters with 10 clusters populated\n",
      "In stage 4: transferred 71 clusters with 10 clusters populated\n",
      "In stage 5: transferred 55 clusters with 10 clusters populated\n",
      "In stage 6: transferred 41 clusters with 10 clusters populated\n",
      "In stage 7: transferred 38 clusters with 10 clusters populated\n",
      "In stage 8: transferred 40 clusters with 10 clusters populated\n",
      "In stage 9: transferred 46 clusters with 10 clusters populated\n",
      "In stage 10: transferred 49 clusters with 10 clusters populated\n",
      "In stage 11: transferred 44 clusters with 10 clusters populated\n",
      "In stage 12: transferred 45 clusters with 10 clusters populated\n",
      "In stage 13: transferred 52 clusters with 10 clusters populated\n",
      "In stage 14: transferred 34 clusters with 10 clusters populated\n",
      "In stage 15: transferred 37 clusters with 10 clusters populated\n",
      "In stage 16: transferred 37 clusters with 10 clusters populated\n",
      "In stage 17: transferred 40 clusters with 10 clusters populated\n",
      "In stage 18: transferred 39 clusters with 10 clusters populated\n",
      "In stage 19: transferred 42 clusters with 10 clusters populated\n",
      "In stage 20: transferred 43 clusters with 10 clusters populated\n",
      "In stage 21: transferred 41 clusters with 10 clusters populated\n",
      "In stage 22: transferred 36 clusters with 10 clusters populated\n",
      "In stage 23: transferred 36 clusters with 10 clusters populated\n",
      "In stage 24: transferred 40 clusters with 10 clusters populated\n",
      "In stage 25: transferred 33 clusters with 10 clusters populated\n",
      "In stage 26: transferred 31 clusters with 10 clusters populated\n",
      "In stage 27: transferred 32 clusters with 10 clusters populated\n",
      "In stage 28: transferred 29 clusters with 10 clusters populated\n",
      "In stage 29: transferred 30 clusters with 10 clusters populated\n",
      "Number of documents per topic : [171  26 294 159  43  21  36 108  69  23]\n",
      "********************\n",
      "********************\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u2010' in position 2: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-cdfaa58fe0df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    184\u001b[0m                    \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                    \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_titles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\geoss_env_win\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u2010' in position 2: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    " if __name__ == \"__main__\":\n",
    "\n",
    "    titles = []\n",
    "    urls = []\n",
    "    abstracts = []\n",
    "    ids = []\n",
    "    abbr_lower = [abbr.lower() for abbr in list(abbreviations.keys())]\n",
    "    original_titles = []\n",
    "    \n",
    "    # get the abstracts from the files \n",
    "    for file in os.listdir(\"biologic_process_abstracts\"):\n",
    "         with open(\"biologic_process_abstracts\\\\\" + file, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                if not \"abstract\" in data:\n",
    "                    continue\n",
    "                ids.append(file)\n",
    "                title = data[\"title\"]\n",
    "                original_titles.append(title)\n",
    "                title = prepareDescription(title, keepwords, abbr_lower)\n",
    "                title = replace_abbrevations(title, abbreviations)\n",
    "                titles.append(title)\n",
    "                \n",
    "                urls.append(data[\"url\"])\n",
    "                \n",
    "                abstract = data[\"abstract\"]\n",
    "                abstract = prepareDescription(abstract, keepwords, abbr_lower)\n",
    "                abstract = replace_abbrevations(abstract, abbreviations)\n",
    "                abstracts.append(abstract)\n",
    "\n",
    "    title_sim = [ [0 for i in range(NUM_LABELS_BIO)] for j in range(len(titles))]\n",
    "    title_id = 0\n",
    "\n",
    "    label_words = [cellular_process_info, development_info, physiological_process_info]\n",
    "\n",
    "    # compare each title to the theme and domain lists\n",
    "    # and obtain the similarity\n",
    "    \n",
    "    # in this case, we have 3 subdomains, so we will compute similarities between titles and \n",
    "    # each of these\n",
    "    \n",
    "    for title in titles:\n",
    "        domain_sim = [0] * NUM_LABELS_BIO\n",
    "        \n",
    "        theme_sim = cosine_similarity_compare(themes, title.split())\n",
    "        \n",
    "        i = 0\n",
    "        for domain in domains:\n",
    "            domain_sim[i] = cosine_similarity_compare(domain.split(), title.split())\n",
    "            i +=1\n",
    "            \n",
    "        for i in range(NUM_LABELS_BIO):\n",
    "            title_sim[title_id][i] += cosine_similarity_compare(label_words[i], title)\n",
    "\n",
    "        for i in range(NUM_LABELS_BIO):\n",
    "            title_sim[title_id][i] = (title_sim[title_id][i] + domain_sim[i] + theme_sim) / 3\n",
    "\n",
    "        title_id += 1\n",
    "\n",
    "    abs_content = [''] * len(titles)\n",
    "\n",
    "    for i in range(len(titles)):\n",
    "        # concat title + abstract, then\n",
    "        # remove words duplicates, this will show some better results\n",
    "        abs_content[i] = (list(dict.fromkeys((titles[i] + ' ' + abstracts[i]).split())))\n",
    "        \n",
    "    T=10\n",
    "    mgp = MovieGroupProcess(K=T, alpha=0.1, beta=0.1, n_iters=30)\n",
    "    vocab = set(x for doc in abs_content for x in doc)\n",
    "    n_terms = len(vocab)\n",
    "    y = mgp.fit(abs_content, n_terms)\n",
    "    \n",
    "\n",
    "    # Save model\n",
    "    with open(\"sttm_v1_bio.model\", \"wb\") as f:\n",
    "        pickle.dump(mgp, f)\n",
    "        f.close()\n",
    "    \n",
    "    doc_count = np.array(mgp.cluster_doc_count)\n",
    "    print('Number of documents per topic :', doc_count)\n",
    "    print('*'*20)# Topics sorted by the number of document they are allocated to\n",
    "    top_index = doc_count.argsort()[(-1*T):][::-1]\n",
    "    \n",
    "    print('*'*20)# Show the top 5 words in term frequency for each cluster\n",
    "    count = 0\n",
    "    \n",
    "    sims_T_L = [ [0 for i in range(NUM_LABELS_BIO)] for j in range(len(mgp.cluster_word_distribution))]\n",
    "    topics = []\n",
    "    \n",
    "    for cluster_dict_per_topic in mgp.cluster_word_distribution:\n",
    "        counter = Counter(cluster_dict_per_topic)\n",
    "\n",
    "        high = counter.most_common(30)\n",
    "\n",
    "        if high == []:\n",
    "            continue\n",
    "\n",
    "        topic_words = [x[0] for x in high]\n",
    "        # the most common words, how are they connected to each predefined topic?\n",
    "        for i in range(len(label_words)):\n",
    "            sims_T_L[count][i] = cosine_similarity_compare(label_words[i], topic_words)\n",
    "\n",
    "        topics.extend(topic_words)\n",
    "        count += 1\n",
    "    \n",
    "    T2 = count\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    sims_D_T = [ [0 for i in range(T2)] for j in range(len(titles))]\n",
    "    \n",
    "    for doc in abs_content:\n",
    "        sims_D_T[count] = mgp.score(doc)\n",
    "        count += 1\n",
    "\n",
    "    # multiply matrices\n",
    "    sims_D_T = np.array(sims_D_T)\n",
    "    sims_T_L = np.array(sims_T_L)\n",
    "    sims_D_L = np.zeros([np.size(sims_D_T, 0), np.size(sims_T_L, 1)])\n",
    "    \n",
    "    for i in range(np.size(sims_D_T, 0)):\n",
    "        for j in range(np.size(sims_T_L, 1)):\n",
    "            if np.count_nonzero(sims_D_T[i, :]) > 0 and np.count_nonzero(sims_T_L[:, j]) > 0:\n",
    "                sims_D_L[i][j] = np.matmul(sims_D_T[i, :], sims_T_L[:, j])\n",
    "                #spatial.distance.cosine(sims_D_T[i, :], sims_T_L[:, j])\n",
    "            else:\n",
    "                sims_D_L[i][j] = 0\n",
    "\n",
    "    no_lines = len(sims_D_L)\n",
    "    no_cols = len(sims_D_L[0])\n",
    "\n",
    "    #which are the words from the concepts which are not considered by the automatic features\n",
    "    topics = list(dict.fromkeys(topics))\n",
    "    \n",
    "    sim_labels = []\n",
    "    i = 0\n",
    "    for label in label_words:\n",
    "        label_list = [w for w in label if w not in topics]\n",
    "        sim_labels.append(label_list)\n",
    "        i += 1\n",
    "\n",
    "    i = 0\n",
    "    sims_D_L_2 = [ [0 for i in range(NUM_LABELS_BIO)] for j in range(len(abs_content))]\n",
    "\n",
    "    # compare these to the corpus and gather similarities\n",
    "    for count in range(len(abs_content)):\n",
    "        sentence  = abs_content[count]\n",
    "        if sentence != []:\n",
    "            for i in range(len(sim_labels)):\n",
    "                label = sim_labels[i]\n",
    "                sims_D_L_2[count][i] = cosine_similarity_compare(label, sentence)\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    # how many lines and columns for sim_D_L_2\n",
    "    sims_D_L_2_lines = len(sims_D_L_2)\n",
    "    sims_D_L_2_cols = len(sims_D_L_2[0])\n",
    "\n",
    "    for i in range(len(sims_D_L)):\n",
    "        maxv = max(sims_D_L[i])\n",
    "        sims_D_L[i] = [x / maxv for x in np.array(sims_D_L[i])]\n",
    "    \n",
    "    with open(SIM_MATRIX_FILE, \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            [writer.writerow(r) for r in sims_D_L]\n",
    "    \n",
    "    with open(ID_LIST_FILE, \"w\") as f:\n",
    "        for idname in ids:\n",
    "            f.write(idname + \"\\n\")\n",
    "    \n",
    "\n",
    "    sim_values_trans = np.array(sims_D_L).transpose()\n",
    "\n",
    "    # print best resources for each topic\n",
    "    cnt = 0\n",
    "    for line in sim_values_trans:\n",
    "        arr = np.array(line)\n",
    "        idx = arr.argsort()[-len(ids):][::-1]\n",
    "        count = 0\n",
    "        cnt += 1\n",
    "        with open(\"results_file\" + str(cnt) + \".txt\", \"w\") as f:\n",
    "            for i in idx:\n",
    "                count += 1\n",
    "                if (count > 10):\n",
    "                    break\n",
    "                if (arr[i] > 0):\n",
    "                    f.write(original_titles[i] + ' ' + urls[i] + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
