{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"__imports__.ipynb\"\n",
    "\n",
    "\n",
    "'''\n",
    "Based on the Google News model and word2vec approach, \n",
    "every word is associated to a vector with num_features entries.\n",
    "The vector associated to the sentence is the average of values for all the words in the\n",
    "sentence that are also contained in the language model\n",
    "'''\n",
    "def avg_feature_vector (sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "'''\n",
    "Build the set of keywords, based on the metadata reords in GeoCatalogue\n",
    "Get all the records, save all the keywords and remove duplicates\n",
    "'''\n",
    "def build_keywords_set (article_keywords, keywords_set):\n",
    "    for key in article_keywords:\n",
    "         #if key is contained by multiple items separated by ','\n",
    "        if ',' in key:\n",
    "            keylist = key.split(', ')\n",
    "            for key in keylist:\n",
    "                if not key.lower() in keywords_set:\n",
    "                    keywords_set.append(key.lower())\n",
    "        else:\n",
    "            if not key.lower() in keywords_set:\n",
    "                keywords_set.append(key.lower())\n",
    "\n",
    "    return keywords_set\n",
    "\n",
    "'''\n",
    "Build a dictionary of keywords, based on the metadata reords in GeoCatalogue\n",
    "Get all the records, save all the keywords and see how often they appear\n",
    "'''\n",
    "def build_keywords_dict (article_keywords, keywords_dict):\n",
    "    for key in article_keywords:\n",
    "        if key:\n",
    "         #if key is contained by multiple items separated by ','\n",
    "            if ',' in key:\n",
    "                keylist = key.split(', ')\n",
    "                for key in keylist:\n",
    "                    if key.lower() in keywords_dict:\n",
    "                        keywords_dict[key.lower()] += 1\n",
    "                    else:\n",
    "                        keywords_dict[key.lower()] = 1\n",
    "            else:\n",
    "                if key.lower() in keywords_dict:\n",
    "                    keywords_dict[key.lower()] += 1\n",
    "                else:\n",
    "                    keywords_dict[key.lower()] = 1\n",
    "    return keywords_dict\n",
    "\n",
    "'''\n",
    "Set the language model, by default it is Google News dataset and it is used\n",
    "to get similarities between words and sentences\n",
    "'''\n",
    "def init_language_model (language_model=LANGUAGE_MODEL):\n",
    "    print(language_model)\n",
    "    model = KeyedVectors.load_word2vec_format(language_model, binary=True)\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    return model, index2word_set\n",
    "\n",
    "\n",
    "'''\n",
    "For all the entries in the list of the keywords, save only the ones similar to\n",
    "the search query\n",
    "'''\n",
    "def get_best_keywords_from_list (keywords_set, sent1, similarity_index):\n",
    "    best_keywords = []\n",
    "\n",
    "    for keyw in keywords_set:\n",
    "        sent2 = avg_feature_vector(keyw, model, num_features=NUM_FEATURES, index2word_set=index2word_set)\n",
    "        all_zeros_sent2 = not np.any(sent2)\n",
    "        if not all_zeros_sent2:\n",
    "            sim = 1 - spatial.distance.cosine(sent1, sent2)\n",
    "            if sim > similarity_index:\n",
    "                best_keywords.append(keyw)\n",
    "\n",
    "    print(\"best keywords for similarity %f: \" %similarity_index)\n",
    "    print(best_keywords)\n",
    "    return best_keywords\n",
    "\n",
    "\n",
    "'''\n",
    "Auxiliary function to get corresponding parts of speech tags between NLP libraries\n",
    "'''\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "'''\n",
    "This function will remove all stop words and punctuation in the text and return\n",
    "a preprocessed variation of the initial description\n",
    "In order to keep the essential words in the description, the following were removed:\n",
    "the stopwords, the non-literal characters, short common words and all the words were\n",
    "transformed to their basic form\n",
    "'''  \n",
    "def prepareDescription(text, keepwords=\"\", abbrevations=\"\"):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", str(text))\n",
    "    \n",
    "    # Split the text words into tokens\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    #Get only lowercase entries\n",
    "    word_tokens = [word.lower() for word in word_tokens]\n",
    "    \n",
    "    # Get all stop words in english.\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    #Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    main_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_tokens]\n",
    "    word_tokens = main_words\n",
    "\n",
    "    # Below list comprehension will return only keywords that are not in stop words\n",
    "    main_words = [word for word in word_tokens if not word in stop_words]\n",
    "    main_words = list( dict.fromkeys(main_words) )\n",
    "\n",
    "    #remove also short words, but keep special words in the list\n",
    "    return ' '.join([word for word in main_words if (len(word) > 3 and DICT.check(word)) or word in keepwords or word in abbrevations])\n",
    "\n",
    "'''\n",
    "Based on the training done by the network, one or multiple labels should be applied\n",
    "on metadata record descriptions that have no keywords attached to them\n",
    "\n",
    "In this case, the starting probability is 0.5, meaning that the algorithm will return\n",
    "all labels with probability higher that 0.5\n",
    "In case there is no one, the function can be called with a smaller probability parameter\n",
    "until at least one label is obtained\n",
    "'''\n",
    "def predict_label(abstract, probability=0.5):\n",
    "\n",
    "    abstract = abstract[0:len(abstract)-1]\n",
    "    #abstract = '\\'' + abstract + '\\''\n",
    "    #escaped = escaped.translate(str.maketrans({\"'\": r\"\\'\", \n",
    "    #                                           \"(\" : r\"\\(\", \")\" : r\"\\)\"}))\n",
    "    #print(escaped)\n",
    "    \n",
    "    #send the labeling command to fasttext application for a record description\n",
    "    #to get the best labels for the given entry\n",
    "    p1 = Popen((\"echo \" + abstract).split(), stdout = PIPE, close_fds=True)\n",
    "    command = (FASTTEXT_PATH + FASTTEXT_COMMAND + \" predict \" + FASTTEXT_PATH + MODEL_NAME + \".bin\" + \" - -1 \" + \n",
    "                str(float(probability)))\n",
    "    print(command)\n",
    "    p2 = Popen(command.split(), stdin = p1.stdout, stdout = subprocess.PIPE,\n",
    "               stderr=subprocess.STDOUT,  close_fds=True)\n",
    "    stdout, stderr = p2.communicate()\n",
    "    out_label = stdout[0:len(stdout)-1].decode(\"utf-8\")\n",
    "    return out_label\n",
    "\n",
    "\n",
    "'''\n",
    "In order to build training data, metadata descriptions are obtained from the GeoCatalogue\n",
    "and the best keywords are selected from the entire list\n",
    "The best keywords are words similar to the search query, that can be found in the metadata\n",
    "'''\n",
    "def build_training_test_database(records, best_keywords):\n",
    "    #read file and see which of the articles contain these keywords\n",
    "    #build a training file\n",
    "    training_file = open(TRAINING_FILE, 'w')\n",
    "    test_file = open(TEST_FILE, 'w')\n",
    "\n",
    "    for rec in records:\n",
    "        #if there are multiple keywords per metadata records, they are separated by ','\n",
    "        article_best_keywords = []\n",
    "        for key in records[rec].subjects:\n",
    "            if ',' in key:\n",
    "                keylist = key.split(', ')\n",
    "                for single_key in keylist:\n",
    "                    #check if the keywords related to the metadata record is in the list of\n",
    "                    #the best keywords, than apped it to the list and write it in the training file\n",
    "                    if single_key.lower() in best_keywords:\n",
    "                        article_best_keywords.append(single_key.lower())\n",
    "            else:\n",
    "                if key.lower() in best_keywords:\n",
    "                    article_best_keywords.append(key.lower())\n",
    "        #remove the new lines\n",
    "        records[rec].abstract = records[rec].abstract.replace('\\n', ' ')\n",
    "        if len(article_best_keywords) != 0:\n",
    "            #replace all spaces in keywords with '-' and write them as labels\n",
    "            for key in article_best_keywords:\n",
    "                key = key.replace(' ', '-')\n",
    "                training_file.write('__label__%s ' % (key))\n",
    "            #write the simplified abstract of the metadata record in the training file\n",
    "            #if there are labels attached to it\n",
    "            training_file.write(prepareDescription(records[rec].abstract) + '\\n')\n",
    "\n",
    "        #write the simplified abstract of the metadata record in the test file\n",
    "        test_file.write(prepareDescription(records[rec].abstract) + '\\n')\n",
    "\n",
    "    training_file.close()\n",
    "    test_file.close()\n",
    "\n",
    "\n",
    "'''\n",
    "Run the classsifer\n",
    "At this point, it i called through bash command and it can be tested for various\n",
    "combinations of parameters\n",
    "'''\n",
    "def train_fasttext_classifer(training_file=TRAINING_FILE):\n",
    "    lr = 0.5\n",
    "    epoch = 25\n",
    "    wordNgrams = 2\n",
    "    \n",
    "    bashCommand = ('rm ' + MODEL_NAME)\n",
    "    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    bashCommand = (FASTTEXT_PATH + FASTTEXT_COMMAND + ' supervised -input ' +\n",
    "                training_file + ' -output ' + FASTTEXT_PATH + MODEL_NAME + ' -lr ' + str(lr) +\n",
    "                ' -epoch ' + str(epoch) + ' -wordNgrams ' + str(wordNgrams))\n",
    "    print(bashCommand)\n",
    "    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "\n",
    "\n",
    "'''\n",
    "After the classifier is trainied, it is run on the records that have no keywords associated\n",
    "in order to provide the most similar entries.\n",
    "The classifier tries to get the most probable matches, starting from a threshold of 0.5 and then\n",
    "it decreases the probability by 0.1 until it gets to a results.\n",
    "In case no result is obtained, then the search query will be marked as a label for that abstract\n",
    "'''\n",
    "def run_classifier_on_data(count, test_file=TEST_FILE):\n",
    "    decr_rate = 0.1\n",
    "    \n",
    "    myfile = LABELED_FILE + str(count)\n",
    "    print(\"fisierul de deschis e %s\" %myfile)\n",
    "    fp=open(myfile, \"w\")\n",
    "    no_labeled_text = []\n",
    "    \n",
    "    #get all the entries in the test file and try to get labels for them \n",
    "    with open(test_file) as f:\n",
    "        mylist = f.readlines()\n",
    "        for abstract in mylist:\n",
    "            out_label = predict_label(abstract)\n",
    "            if out_label == '':\n",
    "                prob = 0.4\n",
    "                while (prob > 0):\n",
    "                    out_label = predict_label(abstract, prob)\n",
    "                    if out_label != '':\n",
    "                        break\n",
    "                    else:\n",
    "                        prob = prob - decr_rate\n",
    "                if prob <= 0:\n",
    "                    # if there is no label, add the search query as label\n",
    "                    out_label = \"__label__\" + SEARCH_QUERY.replace(\" \", \"-\")\n",
    "            fp.write(out_label + \" \" + abstract)\n",
    "    fp.close()\n",
    "\n",
    "\n",
    "'''\n",
    "Show how many entries are in each category\n",
    "In case there are few entries, they will be printed in order\n",
    "to get an example over how the classifier runs.\n",
    "All the final entries can be found in LABELED_FILE\n",
    "'''\n",
    "def show_results(title, count, labeled_file=LABELED_FILE):\n",
    "    text_dict = {}\n",
    "    pattern = r'(__label__(\\w|-)+\\s*)+(\\w\\s)+'\n",
    "    \n",
    "    with open(labeled_file + str(count)) as f:\n",
    "        text_list = f.readlines()\n",
    "        for line in text_list:\n",
    "            match = re.search(pattern, line)\n",
    "            if match is None:\n",
    "                print(\"match is None\")\n",
    "            else:\n",
    "                labels = match.group(0)\n",
    "                text = line.split(labels)[1]\n",
    "                list_of_labels = re.split('__label__', labels)\n",
    "                for label in list_of_labels:\n",
    "                    if label != '':\n",
    "                        mylabel = label.split(' ')[0]\n",
    "                        if mylabel in text_dict:\n",
    "                            text_dict[mylabel].append(text)\n",
    "                        else:\n",
    "                            text_dict[mylabel] = [text]\n",
    "    \n",
    "    print(\"text_dict len %d\" %len(text_dict))\n",
    "    for key in text_dict:\n",
    "        text_dict[key] = len(text_dict[key])\n",
    "        \n",
    "    df = pandas.DataFrame.from_dict(text_dict, orient='index')\n",
    "    ax = df.plot(kind='bar', title=\"sim\"+str(title))\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\n",
    "\n",
    "\n",
    "def calculate_wcss(data):\n",
    "    wcss = []\n",
    "    for n in range(2, 21):\n",
    "        kmeans = KMeans(n_clusters=n)\n",
    "        kmeans.fit(X=data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    return wcss\n",
    "\n",
    "def optimal_number_of_clusters(wcss):\n",
    "    x1, y1 = 2, wcss[0]\n",
    "    x2, y2 = 20, wcss[len(wcss)-1]\n",
    "\n",
    "    distances = []\n",
    "    for i in range(len(wcss)):\n",
    "        x0 = i+2\n",
    "        y0 = wcss[i]\n",
    "        numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)\n",
    "        denominator = sqrt((y2 - y1)**2 + (x2 - x1)**2)\n",
    "        distances.append(numerator/denominator)\n",
    "    \n",
    "    return distances.index(max(distances)) + 2\n",
    "\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    return np.asarray(sent_vec) / numw\n",
    "\n",
    "def lemmatization(texts, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    #https://spacy.io/api/annotation\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.lemma_ in keepwords or token.pos_ in allowed_postags])\n",
    "    \n",
    "    return texts_out\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc])# if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print('Calculating {}-topic model'.format(num_topics))\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "\n",
    "def format_topics_sentences(texts, ldamodel, corpus):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "def replace_abbrevations (text, abbreviations) :\n",
    "\n",
    "    for abbr in list(abbreviations.keys()):\n",
    "        index = 0\n",
    "        index = text.find(abbr.lower(), index)\n",
    "        while index != -1:\n",
    "            #is this part of a word? if there is a letter in front,\n",
    "            #then don't take it into consideration\n",
    "            #at this point, the text has been preproceesed and tere should not be letters in front or\n",
    "            #after the abbreviation\n",
    "            replace = 1\n",
    "            if index != 0:\n",
    "                if text[index-1].isalpha():\n",
    "                    replace = 0\n",
    "            \n",
    "            if replace == 1:\n",
    "                if index + len(abbr) < len(text):\n",
    "                    if text[index + len(abbr)].isalpha():\n",
    "                        replace = 0\n",
    "                    \n",
    "            if replace == 1:\n",
    "                text = text[:index] + abbreviations[abbr] + text[index + len(abbr):]\n",
    "                      \n",
    "            index = text.find(abbr.lower(), index + 1)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def wmd(q1, q2):\n",
    "    q1 = str(q1).lower().split()\n",
    "    q2 = str(q2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    q1 = [w for w in q1 if w not in stop_words]\n",
    "    q2 = [w for w in q2 if w not in stop_words]\n",
    "    return model.wmdistance(q1, q2)\n",
    "    \n",
    "def norm_wmd(q1, q2):\n",
    "    q1 = str(q1).lower().split()\n",
    "    q2 = str(q2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    q1 = [w for w in q1 if w not in stop_words]\n",
    "    q2 = [w for w in q2 if w not in stop_words]\n",
    "    return norm_model.wmdistance(q1, q2)\n",
    "    \n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "All the abbreviations related to energy in the text will be replaced by their corresponding words,\n",
    "so it would be easier to compute the distances\n",
    "\"\"\"\n",
    "\n",
    "def replace_abbrevations (text, abbreviations):\n",
    "\n",
    "    for abbr in list(abbreviations.keys()):\n",
    "        index = 0\n",
    "        index = text.find(abbr.lower(), index)\n",
    "        while index != -1:\n",
    "            #is this part of a word? if there is a letter in front,\n",
    "            #then don't take it into consideration\n",
    "            #at this point, the text has been preproceesed and tere should not be letters in front or\n",
    "            #after the abbreviation\n",
    "            replace = 1\n",
    "            if index != 0:\n",
    "                if text[index-1].isalpha():\n",
    "                    replace = 0\n",
    "            \n",
    "            if replace == 1:\n",
    "                if index + len(abbr) < len(text):\n",
    "                    if text[index + len(abbr)].isalpha():\n",
    "                        replace = 0\n",
    "                    \n",
    "            if replace == 1:\n",
    "                text = text[:index] + abbreviations[abbr] + text[index + len(abbr):]\n",
    "                      \n",
    "            index = text.find(abbr.lower(), index + 1)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
